{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K리그 패스 예측 - V8 (V7 기반 개선)\n",
    "\n",
    "## 현재 상황\n",
    "- V7: Val 13.90\n",
    "- 앙상블 5개: Val 13.21, Test 13.96\n",
    "- 1등: 12.25\n",
    "\n",
    "## 개선 전략\n",
    "1. **Y축 대칭 데이터 증강** - 학습 시 적용 (TTA 효과 내도록)\n",
    "2. **더 큰 모델** - d_model 192, n_layers 5\n",
    "3. **더 많은 에폭 + 강한 정규화**\n",
    "4. **개선된 Loss** - 거리 직접 최적화 추가\n",
    "5. **앙상블 10개** - 다양한 시드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingWarmRestarts\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_DIR = 'open_track1'\n",
    "CACHE_DIR = 'cache'\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'d_model': 192, 'n_heads': 6, 'n_layers': 5, 'd_ff': 384, 'dropout': 0.15, 'max_seq_len': 64, 'batch_size': 64, 'lr': 0.0002, 'weight_decay': 0.02, 'epochs': 60, 'patience': 15, 'use_y_flip_augment': True, 'augment_prob': 0.5, 'use_amp': True, 'num_workers': 0}\n"
     ]
    }
   ],
   "source": [
    "# 설정 - 더 큰 모델\n",
    "CONFIG = {\n",
    "    # 모델 (더 큼)\n",
    "    'd_model': 192,       # 128 → 192\n",
    "    'n_heads': 6,         # 4 → 6\n",
    "    'n_layers': 5,        # 4 → 5\n",
    "    'd_ff': 384,          # 256 → 384\n",
    "    'dropout': 0.15,      # 0.1 → 0.15 (더 강한 정규화)\n",
    "    'max_seq_len': 64,  # 64\n",
    "    \n",
    "    # 학습\n",
    "    'batch_size': 64,     # 128 → 64 (더 큰 모델이라 줄임)\n",
    "    'lr': 2e-4,           # 5e-4 → 3e-4\n",
    "    'weight_decay': 0.02, # 0.01 → 0.02\n",
    "    'epochs': 60,         # 40 → 60\n",
    "    'patience': 15,       # 10 → 15\n",
    "    \n",
    "    # 데이터 증강\n",
    "    'use_y_flip_augment': True,  # Y축 대칭 증강\n",
    "    'augment_prob': 0.5,         # 50% 확률로 증강\n",
    "    \n",
    "    # 기타\n",
    "    'use_amp': True,\n",
    "    'num_workers': 0,\n",
    "}\n",
    "\n",
    "N_SEQ_FEATURES = 22\n",
    "print(f\"Config: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 13097 episodes\n",
      "Val: 2338 episodes\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로딩\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "match_info = pd.read_csv(f'{DATA_DIR}/match_info.csv')\n",
    "\n",
    "all_games = train_df['game_id'].unique()\n",
    "train_games, val_games = train_test_split(all_games, test_size=0.15, random_state=42)\n",
    "train_episodes = train_df[train_df['game_id'].isin(train_games)]['game_episode'].unique()\n",
    "val_episodes = train_df[train_df['game_id'].isin(val_games)]['game_episode'].unique()\n",
    "\n",
    "print(f\"Train: {len(train_episodes)} episodes\")\n",
    "print(f\"Val: {len(val_episodes)} episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 피처 엔지니어링 (Y축 대칭 지원)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sequence_features(episode_df, flip_y=False):\n",
    "    \"\"\"\n",
    "    확장된 시퀀스 피처 (22개)\n",
    "    flip_y=True면 Y좌표를 68-y로 변환\n",
    "    \"\"\"\n",
    "    df = episode_df.copy().reset_index(drop=True)\n",
    "    n = len(df)\n",
    "    \n",
    "    # Y축 반전\n",
    "    if flip_y:\n",
    "        df['start_y'] = 68 - df['start_y']\n",
    "        df['end_y'] = 68 - df['end_y']\n",
    "    \n",
    "    pass_indices = df[df['type_name'] == 'Pass'].index.tolist()\n",
    "    last_pass_idx = pass_indices[-1] if pass_indices else n - 1 # 삼항 연산자\n",
    "    \n",
    "    df['end_x_filled'] = df['end_x'].copy()\n",
    "    df['end_y_filled'] = df['end_y'].copy()\n",
    "    \n",
    "    for i in range(n):  # 결측값 채우기\n",
    "        if pd.isna(df.loc[i, 'end_x_filled']):\n",
    "            df.loc[i, 'end_x_filled'] = df.loc[i, 'start_x']\n",
    "        if pd.isna(df.loc[i, 'end_y_filled']):\n",
    "            df.loc[i, 'end_y_filled'] = df.loc[i, 'start_y']\n",
    "    # 마지막 패스의 end값을 start값으로 대체(end값을 예측하기 위해서)\n",
    "    df.loc[last_pass_idx, 'end_x_filled'] = df.loc[last_pass_idx, 'start_x']\n",
    "    df.loc[last_pass_idx, 'end_y_filled'] = df.loc[last_pass_idx, 'start_y']\n",
    "    \n",
    "    # 마지막 패스 직전에 일어난 패스들\n",
    "    # avg_dx: x좌표 변화량들의 평균, avg_dy: y좌표 변화량들의 평균\n",
    "    # team_attack_angle: 아크탄젠트를 이용한 팀의 공격 방향, 0에 가까울수록 직선적, 절댓값이 클수록 측면 활용 높고, 백패스\n",
    "    # team_attack_strength: 공격 강도, 평균적인 패스 길이\n",
    "    # 든 생각: 피쳐 추출 시에 평균 계산을 많이 하는데, 마지막 패스에 가까울수록 가중치를 줘보자.\n",
    "    passes_before_last = df[(df['type_name'] == 'Pass') & (df.index < last_pass_idx)]\n",
    "    if len(passes_before_last) > 0:\n",
    "        avg_dx = (passes_before_last['end_x_filled'] - passes_before_last['start_x']).mean()\n",
    "        avg_dy = (passes_before_last['end_y_filled'] - passes_before_last['start_y']).mean()\n",
    "        team_attack_angle = np.arctan2(avg_dy, avg_dx) / np.pi\n",
    "        team_attack_strength = np.sqrt(avg_dx**2 + avg_dy**2) / 30\n",
    "    else:\n",
    "        team_attack_angle = 0\n",
    "        team_attack_strength = 0\n",
    "\n",
    "\n",
    "    # if len(passes_before_last) > 0:\n",
    "    #     # 1. 인덱스 기반 가중치 생성 (선형 가중치 예시)\n",
    "    #     # 마지막 패스에 가까울수록(i가 클수록) 큰 값을 가짐\n",
    "    #     weights = np.arange(len(passes_before_last)) + 1 \n",
    "    #     weights = weights / weights.sum()  # 합이 1이 되도록 정규화\n",
    "        \n",
    "    #     # 2. 가중평균 적용 (np.average 사용)\n",
    "    #     dx_values = (passes_before_last['end_x_filled'] - passes_before_last['start_x']).values\n",
    "    #     dy_values = (passes_before_last['end_y_filled'] - passes_before_last['start_y']).values\n",
    "        \n",
    "    #     avg_dx = np.average(dx_values, weights=weights)\n",
    "    #     avg_dy = np.average(dy_values, weights=weights)\n",
    "        \n",
    "    #     # 이후 계산은 동일\n",
    "    #     team_attack_angle = np.arctan2(avg_dy, avg_dx) / np.pi\n",
    "    #     team_attack_strength = np.sqrt(avg_dx**2 + avg_dy**2) / 30\n",
    "    # else:\n",
    "    #     team_attack_angle = 0\n",
    "    #     team_attack_strength = 0\n",
    "    \n",
    "    features = []\n",
    "    for i in range(n):\n",
    "        row = df.iloc[i]\n",
    "        # 마지막 패스인지 확인하는 부호\n",
    "        # 든 생각: 지금은 모든 행에 대해 피쳐를 계산하는데, carry나 recovery 같은 행은 필터링해서 이상치를 제거하는게 낫지 않을까?\n",
    "        is_last_pass = (i == last_pass_idx)\n",
    "        \n",
    "        # 좌표 정규화\n",
    "        start_x = row['start_x'] / 105\n",
    "        start_y = row['start_y'] / 68\n",
    "        \n",
    "        # 한 이벤트 내에서 공 이동(패스, non패스 포함)의 이동거리, 각도, 시간 간격, 속도 계산\n",
    "        if is_last_pass:\n",
    "            end_x, end_y = start_x, start_y\n",
    "            distance, angle, speed = 0, 0, 0\n",
    "        else:\n",
    "            end_x = row['end_x_filled'] / 105\n",
    "            end_y = row['end_y_filled'] / 68\n",
    "            distance = np.sqrt((row['end_x_filled']-row['start_x'])**2 + \n",
    "                              (row['end_y_filled']-row['start_y'])**2) / 50\n",
    "            angle = np.arctan2(row['end_y_filled']-row['start_y'],\n",
    "                              row['end_x_filled']-row['start_x']) / np.pi\n",
    "            td = df.iloc[i+1]['time_seconds'] - row['time_seconds'] if i < n-1 else 1\n",
    "            td = max(td, 0.1)\n",
    "            speed = min(distance * 50 / td, 50) / 50\n",
    "        \n",
    "        # dist_to_goal: 시작 위치에서 골대까지의 거리\n",
    "        # y_from_center: 경기장 세로 중앙으로부터 y좌표가 떨어진 거리(측면 편차)\n",
    "        # x_progress: x좌표가 0으로 부터 떨어진 거리\n",
    "        # position_in_seq: 에피소드 내에서의 행의 위치\n",
    "        # remaining: 에피소드 내에서 남은 행\n",
    "        dist_to_goal = np.sqrt((row['start_x']-105)**2 + (row['start_y']-34)**2) / 120\n",
    "        y_from_center = (row['start_y'] - 34) / 34\n",
    "        x_progress = row['start_x'] / 105\n",
    "        position_in_seq = i / max(n-1, 1)\n",
    "        remaining = (n-1-i) / max(n-1, 1)\n",
    "        \n",
    "        # continuity_x: 연속된 두 이벤트 사이에 x좌표 위치 연결성, 값이 크면 데이터 누락\n",
    "        # continuity_y: 연속된 두 이벤트 사이에 y좌표 위치 연결성, 값이 크면 데이터 누락\n",
    "        # time_diff:연속된 두 이벤트 사이에 시간 간격, 값이 크면 경기가 중단되었거나 한참 뒤에 일어난 플레이\n",
    "        # 값이 작으면: 아주 빠른 템포의 플레이 (예: 원터치 패스)\n",
    "        if i > 0:\n",
    "            continuity_x = (row['start_x'] - df.iloc[i-1]['end_x_filled']) / 30\n",
    "            continuity_y = (row['start_y'] - df.iloc[i-1]['end_y_filled']) / 30\n",
    "            time_diff = (row['time_seconds'] - df.iloc[i-1]['time_seconds']) / 10\n",
    "        else:\n",
    "            continuity_x, continuity_y, time_diff = 0, 0, 0\n",
    "        \n",
    "        feat = [\n",
    "            start_x, start_y, end_x, end_y,\n",
    "            distance, angle, speed,\n",
    "            dist_to_goal, y_from_center, x_progress,\n",
    "            position_in_seq, remaining,\n",
    "            continuity_x, continuity_y, time_diff,\n",
    "            float(row['is_home']),\n",
    "            float(row['type_name'] == 'Pass'),\n",
    "            float(row['type_name'] == 'Carry'),\n",
    "            float(is_last_pass),\n",
    "            team_attack_angle,\n",
    "            team_attack_strength,\n",
    "            len(passes_before_last) / 20,\n",
    "        ]\n",
    "        features.append(feat)\n",
    "    \n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "\n",
    "def get_last_pass_info(episode_df, flip_y=False):\n",
    "    df = episode_df.copy().reset_index(drop=True)\n",
    "    \n",
    "    if flip_y:\n",
    "        df['start_y'] = 68 - df['start_y']\n",
    "        df['end_y'] = 68 - df['end_y']\n",
    "    \n",
    "    passes = df[df['type_name'] == 'Pass']\n",
    "    last_pass = passes.iloc[-1]\n",
    "    \n",
    "    sx, sy = last_pass['start_x'], last_pass['start_y']\n",
    "    \n",
    "    if len(passes) > 1:\n",
    "        pp = passes.iloc[:-1].copy()\n",
    "        pp['end_x'] = pp['end_x'].fillna(pp['start_x'])\n",
    "        pp['end_y'] = pp['end_y'].fillna(pp['start_y'])\n",
    "        avg_dist = np.sqrt((pp['end_x']-pp['start_x'])**2 + (pp['end_y']-pp['start_y'])**2).mean()\n",
    "    else:\n",
    "        avg_dist = 15\n",
    "    \n",
    "    features = np.array([\n",
    "        sx / 105, sy / 68,\n",
    "        np.sqrt((sx-105)**2 + (sy-34)**2) / 120,\n",
    "        np.arctan2(34-sy, 105-sx) / np.pi,\n",
    "        float(last_pass['is_home']),\n",
    "        avg_dist / 30,\n",
    "        len(df) / 50,\n",
    "        (sy - 34) / 34,\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    return features, np.array([sx, sy], dtype=np.float32)\n",
    "\n",
    "\n",
    "def get_meta_features(game_id, period_id, match_info):\n",
    "    match = match_info[match_info['game_id'] == game_id]\n",
    "    if len(match) > 0:\n",
    "        m = match.iloc[0]\n",
    "        return np.array([\n",
    "            (m['home_score'] - m['away_score']) / 5,\n",
    "            (m['home_score'] + m['away_score']) / 10,\n",
    "            m['game_day'] / 38,\n",
    "            period_id / 2\n",
    "        ], dtype=np.float32)\n",
    "    return np.array([0, 0, 0.5, period_id / 2], dtype=np.float32)\n",
    "\n",
    "\n",
    "def get_target(episode_df, start_point, flip_y=False):\n",
    "    df = episode_df.copy()\n",
    "    if flip_y:\n",
    "        df['end_y'] = 68 - df['end_y']\n",
    "    \n",
    "    last_pass = df[df['type_name'] == 'Pass'].iloc[-1]\n",
    "    end_x, end_y = last_pass['end_x'], last_pass['end_y']\n",
    "    \n",
    "    return np.array([\n",
    "        (end_x - start_point[0]) / 50,\n",
    "        (end_y - start_point[1]) / 50\n",
    "    ], dtype=np.float32), np.array([end_x, end_y], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "캐시 로딩: cache/train_v8_aug.pkl\n",
      "캐시 로딩: cache/val_v8.pkl\n",
      "Train: 13097, Val: 2338\n"
     ]
    }
   ],
   "source": [
    "def precompute_features(df, episodes, match_info, config, cache_path=None, include_flip=False):\n",
    "    \"\"\"\n",
    "    피처 사전 계산\n",
    "    include_flip=True면 Y축 반전 버전도 같이 저장\n",
    "    \"\"\"\n",
    "    if cache_path and os.path.exists(cache_path):\n",
    "        print(f\"캐시 로딩: {cache_path}\")\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    print(\"피처 사전 계산 중...\")\n",
    "    precomputed = {}\n",
    "    max_len = config['max_seq_len']\n",
    "    \n",
    "    for ep_name in tqdm(episodes, desc='Precomputing'):\n",
    "        ep_df = df[df['game_episode'] == ep_name].copy()\n",
    "        \n",
    "        # 원본\n",
    "        seq_feat = compute_sequence_features(ep_df, flip_y=False)\n",
    "        seq_len = len(seq_feat)\n",
    "        \n",
    "        if seq_len > max_len:\n",
    "            seq_feat = seq_feat[-max_len:]\n",
    "            mask = np.ones(max_len, dtype=np.float32)\n",
    "        else:\n",
    "            pad_len = max_len - seq_len\n",
    "            seq_feat = np.vstack([np.zeros((pad_len, N_SEQ_FEATURES), dtype=np.float32), seq_feat])\n",
    "            mask = np.concatenate([np.zeros(pad_len), np.ones(seq_len)]).astype(np.float32)\n",
    "        \n",
    "        lp_feat, start_point = get_last_pass_info(ep_df, flip_y=False)\n",
    "        \n",
    "        game_id = ep_df['game_id'].iloc[0]\n",
    "        period_id = ep_df['period_id'].iloc[0]\n",
    "        meta_feat = get_meta_features(game_id, period_id, match_info)\n",
    "        \n",
    "        try:\n",
    "            target_offset, target_abs = get_target(ep_df, start_point, flip_y=False)\n",
    "        except:\n",
    "            target_offset = np.zeros(2, dtype=np.float32)\n",
    "            target_abs = np.zeros(2, dtype=np.float32)\n",
    "        \n",
    "        precomputed[ep_name] = {\n",
    "            'seq_feat': seq_feat,\n",
    "            'mask': mask,\n",
    "            'lp_feat': lp_feat,\n",
    "            'meta_feat': meta_feat,\n",
    "            'start_point': start_point,\n",
    "            'target_offset': target_offset,\n",
    "            'target_abs': target_abs,\n",
    "        }\n",
    "        \n",
    "        # Y축 반전 버전\n",
    "        if include_flip:\n",
    "            seq_feat_flip = compute_sequence_features(ep_df, flip_y=True)\n",
    "            if len(seq_feat_flip) > max_len:\n",
    "                seq_feat_flip = seq_feat_flip[-max_len:]\n",
    "            else:\n",
    "                pad_len = max_len - len(seq_feat_flip)\n",
    "                seq_feat_flip = np.vstack([np.zeros((pad_len, N_SEQ_FEATURES), dtype=np.float32), seq_feat_flip])\n",
    "            \n",
    "            lp_feat_flip, start_point_flip = get_last_pass_info(ep_df, flip_y=True)\n",
    "            target_offset_flip, target_abs_flip = get_target(ep_df, start_point_flip, flip_y=True)\n",
    "            \n",
    "            precomputed[ep_name]['seq_feat_flip'] = seq_feat_flip\n",
    "            precomputed[ep_name]['lp_feat_flip'] = lp_feat_flip\n",
    "            precomputed[ep_name]['start_point_flip'] = start_point_flip\n",
    "            precomputed[ep_name]['target_offset_flip'] = target_offset_flip\n",
    "            precomputed[ep_name]['target_abs_flip'] = target_abs_flip\n",
    "    \n",
    "    if cache_path:\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(precomputed, f)\n",
    "    \n",
    "    return precomputed\n",
    "\n",
    "# 피처 계산 (증강 포함)\n",
    "train_cache = precompute_features(\n",
    "    train_df, train_episodes, match_info, CONFIG,\n",
    "    cache_path=f'{CACHE_DIR}/train_v8_aug.pkl',\n",
    "    include_flip=True\n",
    ")\n",
    "\n",
    "val_cache = precompute_features(\n",
    "    train_df, val_episodes, match_info, CONFIG,\n",
    "    cache_path=f'{CACHE_DIR}/val_v8.pkl',\n",
    "    include_flip=True  # TTA용\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_cache)}, Val: {len(val_cache)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 (증강 지원)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Y축 대칭 증강을 지원하는 데이터셋\n",
    "    \"\"\"\n",
    "    def __init__(self, precomputed, episodes, augment_prob=0.5, is_train=True):\n",
    "        self.precomputed = precomputed\n",
    "        self.episodes = list(episodes)\n",
    "        self.augment_prob = augment_prob\n",
    "        self.is_train = is_train\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.episodes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ep_name = self.episodes[idx]\n",
    "        data = self.precomputed[ep_name]\n",
    "        \n",
    "        # 학습 중 augment_prob 확률로 Y축 반전 사용\n",
    "        use_flip = self.is_train and random.random() < self.augment_prob and 'seq_feat_flip' in data\n",
    "        \n",
    "        if use_flip:\n",
    "            return {\n",
    "                'sequence': torch.from_numpy(data['seq_feat_flip']),\n",
    "                'seq_mask': torch.from_numpy(data['mask']),\n",
    "                'last_pass': torch.from_numpy(data['lp_feat_flip']),\n",
    "                'meta': torch.from_numpy(data['meta_feat']),\n",
    "                'start_point': torch.from_numpy(data['start_point_flip']),\n",
    "                'target_offset': torch.from_numpy(data['target_offset_flip']),\n",
    "                'target_abs': torch.from_numpy(data['target_abs_flip']),\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'sequence': torch.from_numpy(data['seq_feat']),\n",
    "                'seq_mask': torch.from_numpy(data['mask']),\n",
    "                'last_pass': torch.from_numpy(data['lp_feat']),\n",
    "                'meta': torch.from_numpy(data['meta_feat']),\n",
    "                'start_point': torch.from_numpy(data['start_point']),\n",
    "                'target_offset': torch.from_numpy(data['target_offset']),\n",
    "                'target_abs': torch.from_numpy(data['target_abs']),\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 (더 크고 깊게)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:, :x.size(1)])\n",
    "\n",
    "\n",
    "class ImprovedModel(nn.Module):\n",
    "    \"\"\"\n",
    "    V7 기반 개선 모델\n",
    "    - 더 큰 d_model\n",
    "    - 더 깊은 Transformer\n",
    "    - Residual connections 강화\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        d = config['d_model']\n",
    "        dropout = config['dropout']\n",
    "        \n",
    "        # 시퀀스 인코더\n",
    "        self.seq_embedding = nn.Sequential(\n",
    "            nn.Linear(N_SEQ_FEATURES, d),\n",
    "            nn.LayerNorm(d),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.pos_encoding = PositionalEncoding(d, config['max_seq_len'], dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d, nhead=config['n_heads'],\n",
    "            dim_feedforward=config['d_ff'],\n",
    "            dropout=dropout, activation='gelu', batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=config['n_layers'])\n",
    "        \n",
    "        # 마지막 패스 인코더 (더 깊게)\n",
    "        self.lp_encoder = nn.Sequential(\n",
    "            nn.Linear(8, d // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d // 2, d),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d, d),\n",
    "            nn.LayerNorm(d)\n",
    "        )\n",
    "        \n",
    "        # 메타 인코더\n",
    "        self.meta_encoder = nn.Sequential(\n",
    "            nn.Linear(4, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, 64)\n",
    "        )\n",
    "        \n",
    "        # Cross-Attention\n",
    "        self.cross_attn = nn.MultiheadAttention(d, config['n_heads'], \n",
    "                                                dropout=dropout, batch_first=True)\n",
    "        self.cross_norm = nn.LayerNorm(d)\n",
    "        \n",
    "        # Fusion (더 깊게)\n",
    "        fusion_dim = d * 2 + 64   #64\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # 출력 헤드\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, sequence, seq_mask, last_pass, meta):\n",
    "        B = sequence.size(0)\n",
    "        \n",
    "        # Transformer\n",
    "        seq_emb = self.pos_encoding(self.seq_embedding(sequence))\n",
    "        padding_mask = (seq_mask == 0)\n",
    "        encoded = self.transformer(seq_emb, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # 마지막 토큰\n",
    "        lengths = seq_mask.sum(dim=1).long().clamp(min=1) - 1\n",
    "        seq_out = encoded[torch.arange(B, device=encoded.device), lengths]\n",
    "        \n",
    "        # Cross-Attention\n",
    "        lp_emb = self.lp_encoder(last_pass)\n",
    "        cross_out, _ = self.cross_attn(lp_emb.unsqueeze(1), encoded, encoded,\n",
    "                                       key_padding_mask=padding_mask)\n",
    "        cross_out = self.cross_norm(cross_out.squeeze(1) + lp_emb)\n",
    "        \n",
    "        # Meta\n",
    "        meta_emb = self.meta_encoder(meta)\n",
    "        \n",
    "        # Fusion\n",
    "        combined = torch.cat([seq_out, cross_out, meta_emb], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        return self.regressor(fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 손실 함수 (개선)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    개선된 손실 함수\n",
    "    - MSE + Smooth L1\n",
    "    - 방향 손실 (cosine)\n",
    "    - 거리 손실 (직접 최적화)\n",
    "    \"\"\"\n",
    "    def __init__(self, direction_weight=0.2, distance_weight=0.3):\n",
    "        super().__init__()\n",
    "        self.direction_weight = direction_weight\n",
    "        self.distance_weight = distance_weight\n",
    "    \n",
    "    def forward(self, pred_offset, target_offset, start_point, target_abs):\n",
    "        # 1. MSE + Smooth L1\n",
    "        mse_loss = F.mse_loss(pred_offset, target_offset)\n",
    "        smooth_loss = F.smooth_l1_loss(pred_offset, target_offset)\n",
    "        \n",
    "        # 2. 방향 손실\n",
    "        pred_norm = torch.norm(pred_offset, dim=1, keepdim=True).clamp(min=1e-8)\n",
    "        target_norm = torch.norm(target_offset, dim=1, keepdim=True).clamp(min=1e-8)\n",
    "        cos_sim = (pred_offset * target_offset).sum(dim=1) / (pred_norm.squeeze() * target_norm.squeeze() + 1e-8)\n",
    "        direction_loss = (1 - cos_sim).mean()\n",
    "        \n",
    "        # 3. 거리 손실 (절대 좌표로 직접 계산)\n",
    "        pred_abs = start_point + pred_offset * 50\n",
    "        dist_error = torch.sqrt(((pred_abs - target_abs) ** 2).sum(dim=1) + 1e-8)\n",
    "        distance_loss = dist_error.mean() / 50  # 정규화\n",
    "        \n",
    "        # 총 손실\n",
    "        total_loss = (\n",
    "            mse_loss + 0.5 * smooth_loss +\n",
    "            self.direction_weight * direction_loss +\n",
    "            self.distance_weight * distance_loss\n",
    "        )\n",
    "        \n",
    "        return total_loss, dist_error.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_model(config, train_cache, val_cache, train_episodes, val_episodes, \n",
    "                    device, seed, model_name='model'):\n",
    "    \"\"\"단일 모델 학습\"\"\"\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # 데이터셋\n",
    "    train_ds = AugmentedDataset(train_cache, train_episodes, \n",
    "                                augment_prob=config['augment_prob'], is_train=True)\n",
    "    val_ds = AugmentedDataset(val_cache, val_episodes, \n",
    "                              augment_prob=0, is_train=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=config['batch_size'], \n",
    "                             shuffle=True, num_workers=config['num_workers'], pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=config['batch_size'],\n",
    "                           shuffle=False, num_workers=config['num_workers'], pin_memory=True)\n",
    "    \n",
    "    # 모델\n",
    "    model = ImprovedModel(config).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=config['lr'],\n",
    "                          total_steps=len(train_loader) * config['epochs'], pct_start=0.1)\n",
    "    criterion = ImprovedLoss(direction_weight=0.2, distance_weight=0.3)\n",
    "    scaler = GradScaler() if config['use_amp'] else None\n",
    "    \n",
    "    best_val = float('inf')\n",
    "    best_state = None\n",
    "    patience_cnt = 0\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {model_name} (seed={seed})\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_dist = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}', leave=False):\n",
    "            seq = batch['sequence'].to(device)\n",
    "            mask = batch['seq_mask'].to(device)\n",
    "            lp = batch['last_pass'].to(device)\n",
    "            meta = batch['meta'].to(device)\n",
    "            start = batch['start_point'].to(device)\n",
    "            target_offset = batch['target_offset'].to(device)\n",
    "            target_abs = batch['target_abs'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if config['use_amp']:\n",
    "                with autocast():\n",
    "                    pred = model(seq, mask, lp, meta)\n",
    "                    loss, dist = criterion(pred, target_offset, start, target_abs)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                pred = model(seq, mask, lp, meta)\n",
    "                loss, dist = criterion(pred, target_offset, start, target_abs)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            scheduler.step()\n",
    "            train_dist += dist\n",
    "        \n",
    "        train_dist /= len(train_loader)\n",
    "        \n",
    "        # Validate (with TTA)\n",
    "        model.eval()\n",
    "        val_dist = 0\n",
    "        val_dist_tta = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                seq = batch['sequence'].to(device)\n",
    "                mask = batch['seq_mask'].to(device)\n",
    "                lp = batch['last_pass'].to(device)\n",
    "                meta = batch['meta'].to(device)\n",
    "                start = batch['start_point'].to(device)\n",
    "                target_abs = batch['target_abs'].to(device)\n",
    "                \n",
    "                # 원본 예측\n",
    "                pred = model(seq, mask, lp, meta)\n",
    "                pred_abs = start + pred * 50\n",
    "                dist = torch.sqrt(((pred_abs - target_abs) ** 2).sum(dim=1) + 1e-8).mean()\n",
    "                val_dist += dist.item()\n",
    "        \n",
    "        val_dist /= len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d} | Train: {train_dist:.2f} | Val: {val_dist:.2f}\", end='')\n",
    "        \n",
    "        if val_dist < best_val:\n",
    "            best_val = val_dist\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_cnt = 0\n",
    "            print(f\" ← Best!\")\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            print()\n",
    "        \n",
    "        if patience_cnt >= config['patience']:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    torch.save(best_state, f'{model_name}.pth')\n",
    "    \n",
    "    print(f\"\\n{model_name} 완료! Best Val: {best_val:.2f}\")\n",
    "    \n",
    "    return model, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training v8_model_1 (seed=42)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train: 18.47 | Val: 18.16 ← Best!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2 | Train: 17.85 | Val: 17.83 ← Best!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3 | Train: 17.36 | Val: 17.65 ← Best!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 단일 모델 먼저 테스트\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model, val_dist \u001b[38;5;241m=\u001b[39m train_one_model(\n\u001b[0;32m      3\u001b[0m     CONFIG, train_cache, val_cache, train_episodes, val_episodes,\n\u001b[0;32m      4\u001b[0m     DEVICE, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv8_model_1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m단일 모델 Val: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_dist\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m기존 V7: 13.90\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 56\u001b[0m, in \u001b[0;36mtrain_one_model\u001b[1;34m(config, train_cache, val_cache, train_episodes, val_episodes, device, seed, model_name)\u001b[0m\n\u001b[0;32m     54\u001b[0m     scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m     55\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[0;32m     57\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\LeeJaeBin\\anaconda3\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\LeeJaeBin\\anaconda3\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\LeeJaeBin\\anaconda3\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# 단일 모델 먼저 테스트\n",
    "model, val_dist = train_one_model(\n",
    "    CONFIG, train_cache, val_cache, train_episodes, val_episodes,\n",
    "    DEVICE, seed=42, model_name='v8_model_1'\n",
    ")\n",
    "\n",
    "print(f\"\\n단일 모델 Val: {val_dist:.2f}\")\n",
    "print(f\"기존 V7: 13.90\")\n",
    "print(f\"개선: {13.90 - val_dist:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 앙상블 (10개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training v8_model_1 (seed=42)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train: 18.47 | Val: 18.16 ← Best!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2 | Train: 17.85 | Val: 17.83 ← Best!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3 | Train: 17.36 | Val: 17.65 ← Best!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4 | Train: 17.11 | Val: 16.88 ← Best!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m val_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, seed \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(SEEDS):\n\u001b[1;32m----> 7\u001b[0m     model, val_dist \u001b[38;5;241m=\u001b[39m train_one_model(\n\u001b[0;32m      8\u001b[0m         CONFIG, train_cache, val_cache, train_episodes, val_episodes,\n\u001b[0;32m      9\u001b[0m         DEVICE, seed\u001b[38;5;241m=\u001b[39mseed, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv8_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m     )\n\u001b[0;32m     11\u001b[0m     models\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m     12\u001b[0m     val_scores\u001b[38;5;241m.\u001b[39mappend(val_dist)\n",
      "Cell \u001b[1;32mIn[9], line 51\u001b[0m, in \u001b[0;36mtrain_one_model\u001b[1;34m(config, train_cache, val_cache, train_episodes, val_episodes, device, seed, model_name)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_amp\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m---> 51\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(seq, mask, lp, meta)\n\u001b[0;32m     52\u001b[0m         loss, dist \u001b[38;5;241m=\u001b[39m criterion(pred, target_offset, start, target_abs)\n\u001b[0;32m     53\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\LeeJaeBin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LeeJaeBin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[7], line 103\u001b[0m, in \u001b[0;36mImprovedModel.forward\u001b[1;34m(self, sequence, seq_mask, last_pass, meta)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Cross-Attention\u001b[39;00m\n\u001b[0;32m    102\u001b[0m lp_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlp_encoder(last_pass)\n\u001b[1;32m--> 103\u001b[0m cross_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn(lp_emb\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), encoded, encoded,\n\u001b[0;32m    104\u001b[0m                                key_padding_mask\u001b[38;5;241m=\u001b[39mpadding_mask)\n\u001b[0;32m    105\u001b[0m cross_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_norm(cross_out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m lp_emb)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Meta\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LeeJaeBin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LeeJaeBin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\LeeJaeBin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1373\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1347\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1348\u001b[0m         query,\n\u001b[0;32m   1349\u001b[0m         key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1370\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m   1371\u001b[0m     )\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1373\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1374\u001b[0m         query,\n\u001b[0;32m   1375\u001b[0m         key,\n\u001b[0;32m   1376\u001b[0m         value,\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim,\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight,\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k,\n\u001b[0;32m   1382\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v,\n\u001b[0;32m   1383\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[0;32m   1384\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[0;32m   1385\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1386\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m   1387\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[0;32m   1388\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[0;32m   1389\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[0;32m   1390\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m   1391\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1392\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m   1393\u001b[0m     )\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\LeeJaeBin\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:6230\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   6226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[0;32m   6227\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m   6228\u001b[0m         in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   6229\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 6230\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n\u001b[0;32m   6231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6232\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m   6233\u001b[0m         q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   6234\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\LeeJaeBin\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5631\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[1;34m(q, k, v, w, b)\u001b[0m\n\u001b[0;32m   5629\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5630\u001b[0m     b_q, b_kv \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39msplit([E, E \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m-> 5631\u001b[0m q_proj \u001b[38;5;241m=\u001b[39m linear(q, w_q, b_q)\n\u001b[0;32m   5632\u001b[0m kv_proj \u001b[38;5;241m=\u001b[39m linear(k, w_kv, b_kv)\n\u001b[0;32m   5633\u001b[0m \u001b[38;5;66;03m# reshape to 2, E and not E, 2 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 앙상블 학습 (10개)\n",
    "SEEDS = [42, 123, 456, 789, 2024, 777, 888, 999, 1234, 5678]\n",
    "models = []\n",
    "val_scores = []\n",
    "\n",
    "for i, seed in enumerate(SEEDS):\n",
    "    model, val_dist = train_one_model(\n",
    "        CONFIG, train_cache, val_cache, train_episodes, val_episodes,\n",
    "        DEVICE, seed=seed, model_name=f'v8_model_{i+1}'\n",
    "    )\n",
    "    models.append(model)\n",
    "    val_scores.append(val_dist)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() if DEVICE.type == 'cuda' else None\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"개별 모델 Val:\")\n",
    "for i, (seed, score) in enumerate(zip(SEEDS, val_scores)):\n",
    "    print(f\"  Model {i+1} (seed={seed}): {score:.2f}\")\n",
    "print(f\"평균: {np.mean(val_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble Val: 100%|██████████| 2338/2338 [02:56<00:00, 13.26it/s]\n",
      "Ensemble Val: 100%|██████████| 2338/2338 [05:51<00:00,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "앙상블 결과:\n",
      "  앙상블 (No TTA): 13.49\n",
      "  앙상블 (TTA):    13.51\n",
      "\n",
      "기존 최고 (앙상블 5개): 13.21\n",
      "1등: 12.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 앙상블 Val 검증\n",
    "@torch.no_grad()\n",
    "def validate_ensemble(models, val_cache, val_episodes, device, use_tta=True):\n",
    "    for m in models:\n",
    "        m.eval()\n",
    "    \n",
    "    all_dists = []\n",
    "    \n",
    "    for ep_name in tqdm(val_episodes, desc='Ensemble Val'):\n",
    "        data = val_cache[ep_name]\n",
    "        \n",
    "        # 원본 피처\n",
    "        seq = torch.from_numpy(data['seq_feat']).unsqueeze(0).to(device)\n",
    "        mask = torch.from_numpy(data['mask']).unsqueeze(0).to(device)\n",
    "        lp = torch.from_numpy(data['lp_feat']).unsqueeze(0).to(device)\n",
    "        meta = torch.from_numpy(data['meta_feat']).unsqueeze(0).to(device)\n",
    "        start = data['start_point']\n",
    "        target = data['target_abs']\n",
    "        \n",
    "        # 원본 앙상블 예측\n",
    "        preds_orig = []\n",
    "        for model in models:\n",
    "            pred = model(seq, mask, lp, meta)[0].cpu().numpy()\n",
    "            pred_abs = start + pred * 50\n",
    "            preds_orig.append(pred_abs)\n",
    "        ensemble_orig = np.mean(preds_orig, axis=0)\n",
    "        \n",
    "        if use_tta and 'seq_feat_flip' in data:\n",
    "            # 반전 피처\n",
    "            seq_flip = torch.from_numpy(data['seq_feat_flip']).unsqueeze(0).to(device)\n",
    "            lp_flip = torch.from_numpy(data['lp_feat_flip']).unsqueeze(0).to(device)\n",
    "            start_flip = data['start_point_flip']\n",
    "            \n",
    "            preds_flip = []\n",
    "            for model in models:\n",
    "                pred = model(seq_flip, mask, lp_flip, meta)[0].cpu().numpy()\n",
    "                pred_abs = start_flip + pred * 50\n",
    "                pred_abs[1] = 68 - pred_abs[1]  # Y 다시 반전\n",
    "                preds_flip.append(pred_abs)\n",
    "            ensemble_flip = np.mean(preds_flip, axis=0)\n",
    "            \n",
    "            # TTA 평균\n",
    "            final_pred = (ensemble_orig + ensemble_flip) / 2\n",
    "        else:\n",
    "            final_pred = ensemble_orig\n",
    "        \n",
    "        dist = np.sqrt((final_pred[0] - target[0])**2 + (final_pred[1] - target[1])**2)\n",
    "        all_dists.append(dist)\n",
    "    \n",
    "    return np.mean(all_dists)\n",
    "\n",
    "# 앙상블 검증\n",
    "ensemble_val = validate_ensemble(models, val_cache, val_episodes, DEVICE, use_tta=False)\n",
    "ensemble_val_tta = validate_ensemble(models, val_cache, val_episodes, DEVICE, use_tta=True)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"앙상블 결과:\")\n",
    "print(f\"  앙상블 (No TTA): {ensemble_val:.2f}\")\n",
    "print(f\"  앙상블 (TTA):    {ensemble_val_tta:.2f}\")\n",
    "print(f\"\\n기존 최고 (앙상블 5개): 13.21\")\n",
    "print(f\"1등: 12.25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 2414/2414 [06:32<00:00,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "제출 파일 저장!\n",
      "  game_episode      end_x      end_y\n",
      "0     153363_1  60.445557  22.038307\n",
      "1     153363_2  24.488165  47.759064\n",
      "2     153363_6  40.602531  61.833672\n",
      "3     153363_7  60.556396  11.537536\n",
      "4     153363_8  81.772652   9.559481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def predict_test(models, test_df, match_info, config, device, use_tta=True, test_dir=f'{DATA_DIR}/test'):\n",
    "    for m in models:\n",
    "        m.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    max_len = config['max_seq_len']\n",
    "    \n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc='Test'):\n",
    "        path = row['path'].replace('./test/', f'{test_dir}/')\n",
    "        ep_df = pd.read_csv(path)\n",
    "        ep_name = row['game_episode']\n",
    "        \n",
    "        # 원본 피처\n",
    "        seq_feat = compute_sequence_features(ep_df, flip_y=False)\n",
    "        seq_len = len(seq_feat)\n",
    "        if seq_len > max_len:\n",
    "            seq_feat = seq_feat[-max_len:]\n",
    "            mask = np.ones(max_len, dtype=np.float32)\n",
    "        else:\n",
    "            pad_len = max_len - seq_len\n",
    "            seq_feat = np.vstack([np.zeros((pad_len, N_SEQ_FEATURES), dtype=np.float32), seq_feat])\n",
    "            mask = np.concatenate([np.zeros(pad_len), np.ones(seq_len)]).astype(np.float32)\n",
    "        \n",
    "        lp_feat, start = get_last_pass_info(ep_df, flip_y=False)\n",
    "        meta_feat = get_meta_features(ep_df['game_id'].iloc[0], ep_df['period_id'].iloc[0], match_info)\n",
    "        \n",
    "        seq = torch.from_numpy(seq_feat).unsqueeze(0).to(device)\n",
    "        mask_t = torch.from_numpy(mask).unsqueeze(0).to(device)\n",
    "        lp = torch.from_numpy(lp_feat).unsqueeze(0).to(device)\n",
    "        meta = torch.from_numpy(meta_feat).unsqueeze(0).to(device)\n",
    "        \n",
    "        # 원본 앙상블\n",
    "        preds_orig = []\n",
    "        for model in models:\n",
    "            pred = model(seq, mask_t, lp, meta)[0].cpu().numpy()\n",
    "            pred_abs = start + pred * 50\n",
    "            preds_orig.append(pred_abs)\n",
    "        ensemble_orig = np.mean(preds_orig, axis=0)\n",
    "        \n",
    "        if use_tta:\n",
    "            # 반전 피처\n",
    "            seq_feat_flip = compute_sequence_features(ep_df, flip_y=True)\n",
    "            if len(seq_feat_flip) > max_len:\n",
    "                seq_feat_flip = seq_feat_flip[-max_len:]\n",
    "            else:\n",
    "                pad_len = max_len - len(seq_feat_flip)\n",
    "                seq_feat_flip = np.vstack([np.zeros((pad_len, N_SEQ_FEATURES), dtype=np.float32), seq_feat_flip])\n",
    "            \n",
    "            lp_feat_flip, start_flip = get_last_pass_info(ep_df, flip_y=True)\n",
    "            \n",
    "            seq_flip = torch.from_numpy(seq_feat_flip).unsqueeze(0).to(device)\n",
    "            lp_flip = torch.from_numpy(lp_feat_flip).unsqueeze(0).to(device)\n",
    "            \n",
    "            preds_flip = []\n",
    "            for model in models:\n",
    "                pred = model(seq_flip, mask_t, lp_flip, meta)[0].cpu().numpy()\n",
    "                pred_abs = start_flip + pred * 50\n",
    "                pred_abs[1] = 68 - pred_abs[1]\n",
    "                preds_flip.append(pred_abs)\n",
    "            ensemble_flip = np.mean(preds_flip, axis=0)\n",
    "            \n",
    "            final_pred = (ensemble_orig + ensemble_flip) / 2\n",
    "        else:\n",
    "            final_pred = ensemble_orig\n",
    "        \n",
    "        predictions.append({\n",
    "            'game_episode': ep_name,\n",
    "            'end_x': np.clip(final_pred[0], 0, 105),\n",
    "            'end_y': np.clip(final_pred[1], 0, 68)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(predictions)\n",
    "\n",
    "# Test 예측\n",
    "submission = predict_test(models, test_df, match_info, CONFIG, DEVICE, use_tta=True)\n",
    "submission.to_csv('submission_v8.csv', index=False)\n",
    "\n",
    "print(\"\\n제출 파일 저장!\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "V8 최종 요약\n",
      "============================================================\n",
      "\n",
      "모델 구조:\n",
      "  d_model: 192\n",
      "  n_layers: 5\n",
      "  n_heads: 6\n",
      "  dropout: 0.15\n",
      "\n",
      "개선사항:\n",
      "  ✓ Y축 대칭 데이터 증강 (학습 시 50%)\n",
      "  ✓ 더 큰 모델 (128→192, 4→5 layers)\n",
      "  ✓ 강한 정규화 (dropout 0.15)\n",
      "  ✓ 거리 직접 최적화 Loss\n",
      "  ✓ 앙상블 10개 + TTA\n",
      "\n",
      "결과:\n",
      "  개별 모델 평균: 13.60\n",
      "  앙상블 (No TTA): 13.49\n",
      "  앙상블 (TTA): 13.51\n",
      "\n",
      "비교:\n",
      "  V7 단일: 13.90\n",
      "  기존 앙상블 5개: 13.21\n",
      "  1등: 12.25\n"
     ]
    }
   ],
   "source": [
    "# 최종 요약\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"V8 최종 요약\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n모델 구조:\")\n",
    "print(f\"  d_model: {CONFIG['d_model']}\")\n",
    "print(f\"  n_layers: {CONFIG['n_layers']}\")\n",
    "print(f\"  n_heads: {CONFIG['n_heads']}\")\n",
    "print(f\"  dropout: {CONFIG['dropout']}\")\n",
    "print(f\"\\n개선사항:\")\n",
    "print(f\"  ✓ Y축 대칭 데이터 증강 (학습 시 50%)\")\n",
    "print(f\"  ✓ 더 큰 모델 (128→192, 4→5 layers)\")\n",
    "print(f\"  ✓ 강한 정규화 (dropout 0.15)\")\n",
    "print(f\"  ✓ 거리 직접 최적화 Loss\")\n",
    "print(f\"  ✓ 앙상블 10개 + TTA\")\n",
    "print(f\"\\n결과:\")\n",
    "print(f\"  개별 모델 평균: {np.mean(val_scores):.2f}\")\n",
    "print(f\"  앙상블 (No TTA): {ensemble_val:.2f}\")\n",
    "print(f\"  앙상블 (TTA): {ensemble_val_tta:.2f}\")\n",
    "print(f\"\\n비교:\")\n",
    "print(f\"  V7 단일: 13.90\")\n",
    "print(f\"  기존 앙상블 5개: 13.21\")\n",
    "print(f\"  1등: 12.25\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
